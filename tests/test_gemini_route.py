"""Tests for gemini_route.py â€” template-based task routing.

Generated by Gemini Flash via test-gen template, reviewed and fixed by Claude.
"""

import json
import subprocess
import sys
from pathlib import Path

import pytest

sys.path.insert(0, str(Path(__file__).parent.parent))

from gemini_route import (
    EVAL_LOG,
    TEMPLATE_DIR,
    TEMPLATES,
    fill_template,
    load_template,
    log_eval,
    validate_output,
)


# --- Fixtures ---

@pytest.fixture
def cleanup_eval_log():
    """Backup and restore eval log around tests."""
    backup = None
    if EVAL_LOG.exists():
        backup = EVAL_LOG.read_text()
        EVAL_LOG.unlink()
    yield
    if EVAL_LOG.exists():
        EVAL_LOG.unlink()
    if backup is not None:
        EVAL_LOG.write_text(backup)


# --- TEMPLATES dict structure ---

def test_templates_has_required_keys():
    for cat, meta in TEMPLATES.items():
        assert 'skills' in meta, f"{cat} missing 'skills'"
        assert 'ollama' in meta, f"{cat} missing 'ollama'"
        assert 'savings' in meta, f"{cat} missing 'savings'"
        assert 'desc' in meta, f"{cat} missing 'desc'"
        assert 'priority' in meta, f"{cat} missing 'priority'"


def test_templates_skills_are_lists():
    for cat, meta in TEMPLATES.items():
        assert isinstance(meta['skills'], list), f"{cat} skills is not a list"
        assert len(meta['skills']) > 0, f"{cat} has empty skills list"


def test_templates_savings_are_positive():
    for cat, meta in TEMPLATES.items():
        assert meta['savings'] > 0, f"{cat} has non-positive savings"


def test_templates_priority_in_range():
    for cat, meta in TEMPLATES.items():
        assert meta['priority'] in (1, 2, 3), f"{cat} has invalid priority {meta['priority']}"


def test_every_template_has_file():
    for cat in TEMPLATES:
        path = TEMPLATE_DIR / f'{cat}.txt'
        assert path.exists(), f"Missing template file for '{cat}': {path}"


# --- load_template ---

def test_load_template_valid():
    template = load_template('test-gen')
    assert isinstance(template, str)
    assert len(template) > 0
    assert '{context}' in template


def test_load_template_invalid_raises():
    with pytest.raises(FileNotFoundError):
        load_template('nonexistent-category-xyz')


def test_load_template_all_categories():
    for cat in TEMPLATES:
        template = load_template(cat)
        assert len(template) > 50, f"Template '{cat}' seems too short"


# --- fill_template ---

def test_fill_template_replaces_context():
    result = fill_template("Before {context} after", context="INSERTED")
    assert result == "Before INSERTED after"


def test_fill_template_replaces_task():
    result = fill_template("Do: {task}", task="something")
    assert result == "Do: something"


def test_fill_template_replaces_both():
    result = fill_template("{context}\n{task}", context="CODE", task="FIX IT")
    assert result == "CODE\nFIX IT"


def test_fill_template_empty_context_replaces_with_empty():
    result = fill_template("X{context}Y", context="")
    assert result == "XY"


def test_fill_template_no_placeholders_unchanged():
    result = fill_template("No placeholders here", context="ignored", task="ignored")
    assert result == "No placeholders here"


def test_fill_template_missing_task_replaces_with_empty():
    result = fill_template("Task: {task}", context="ctx")
    assert result == "Task: "


# --- log_eval ---

def test_log_eval_creates_entry(cleanup_eval_log):
    log_eval('test-gen', 'gemini', 100, 200, 500, True)
    assert EVAL_LOG.exists()
    entry = json.loads(EVAL_LOG.read_text().strip())
    assert entry['category'] == 'test-gen'
    assert entry['model'] == 'gemini'
    assert entry['success'] is True
    assert entry['est_tokens_saved'] == 3500


def test_log_eval_zero_values(cleanup_eval_log):
    log_eval('test-gen', 'ollama', 0, 0, 0, False)
    entry = json.loads(EVAL_LOG.read_text().strip())
    assert entry['success'] is False
    assert entry['context_chars'] == 0


def test_log_eval_unknown_category_uses_zero_savings(cleanup_eval_log):
    log_eval('unknown-cat', 'gemini', 10, 20, 100, True)
    entry = json.loads(EVAL_LOG.read_text().strip())
    assert entry['est_tokens_saved'] == 0


def test_log_eval_appends_multiple(cleanup_eval_log):
    log_eval('test-gen', 'gemini', 100, 200, 500, True)
    log_eval('css-draft', 'ollama', 50, 100, 300, True)
    lines = EVAL_LOG.read_text().strip().split('\n')
    assert len(lines) == 2


# --- Path traversal security ---

def test_cli_blocks_path_traversal():
    result = subprocess.run(
        [sys.executable, str(Path(__file__).parent.parent / 'gemini_route.py'),
         'test-gen', '--context', '/etc/passwd'],
        capture_output=True, text=True
    )
    assert result.returncode != 0
    assert "must be under" in result.stderr


def test_cli_blocks_sensitive_path():
    result = subprocess.run(
        [sys.executable, str(Path(__file__).parent.parent / 'gemini_route.py'),
         'test-gen', '--context', '/tmp/something'],
        capture_output=True, text=True
    )
    assert result.returncode != 0


def test_cli_allows_development_path():
    """Allowed path should not error on path check (may error on missing file)."""
    result = subprocess.run(
        [sys.executable, str(Path(__file__).parent.parent / 'gemini_route.py'),
         'test-gen', '--context', str(Path.home() / 'Development/tools/gemini_route.py')],
        capture_output=True, text=True,
        timeout=30
    )
    # Should either succeed (if API key set) or fail on API call, not on path check
    assert "must be under" not in result.stderr


# --- CLI validation ---

def test_cli_unknown_category():
    result = subprocess.run(
        [sys.executable, str(Path(__file__).parent.parent / 'gemini_route.py'),
         'totally-fake-category'],
        capture_output=True, text=True
    )
    assert result.returncode != 0
    assert "Unknown category" in result.stderr


def test_cli_list_flag():
    result = subprocess.run(
        [sys.executable, str(Path(__file__).parent.parent / 'gemini_route.py'),
         '--list'],
        capture_output=True, text=True
    )
    assert result.returncode == 0
    assert 'test-gen' in result.stdout


def test_cli_coverage_flag():
    result = subprocess.run(
        [sys.executable, str(Path(__file__).parent.parent / 'gemini_route.py'),
         '--coverage'],
        capture_output=True, text=True
    )
    assert result.returncode == 0
    assert 'Coverage' in result.stdout


# --- Output validation ---

def test_validate_output_empty():
    is_valid, reason = validate_output('', 'test-gen')
    assert not is_valid
    assert reason == 'empty_output'


def test_validate_output_too_short():
    is_valid, reason = validate_output('hi', 'test-gen')
    assert not is_valid
    assert 'too_short' in reason


def test_validate_output_error_response():
    is_valid, reason = validate_output("ERROR: something went wrong with the request", 'css-draft')
    assert not is_valid
    assert 'error_response' in reason


def test_validate_output_apology_response():
    is_valid, reason = validate_output("I'm sorry, I can't help with that. " * 5, 'test-gen')
    assert not is_valid
    assert 'error_response' in reason


def test_validate_output_test_gen_no_functions():
    is_valid, reason = validate_output("Here are some tests:\n\nassert True\nassert 1 == 1\n" * 5, 'test-gen')
    assert not is_valid
    assert reason == 'no_test_functions'


def test_validate_output_test_gen_valid():
    code = "import pytest\n\ndef test_example():\n    assert True\n\ndef test_another():\n    assert 1 == 1\n" * 3
    is_valid, reason = validate_output(code, 'test-gen')
    assert is_valid
    assert reason == 'ok'


def test_validate_output_css_draft_no_blocks():
    is_valid, reason = validate_output("Change the color to blue and make the font bigger. " * 5, 'css-draft')
    assert not is_valid
    assert reason == 'no_css_blocks'


def test_validate_output_css_draft_valid():
    css = ".container { color: blue; font-size: 16px; }\n.header { background: red; }\n" * 3
    is_valid, reason = validate_output(css, 'css-draft')
    assert is_valid


def test_validate_output_generic_valid():
    text = "This is a perfectly normal output that should pass validation without any issues. " * 5
    is_valid, reason = validate_output(text, 'kb-summarize')
    assert is_valid
    assert reason == 'ok'
